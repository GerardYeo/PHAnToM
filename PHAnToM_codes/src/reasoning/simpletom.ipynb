{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import argparse\n",
    "import random\n",
    "import evaluate\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import fantom_dataset_loader as loader\n",
    "from fantom_agents import FlanT5Agent, FlanUL2Agent, MistralAIAgent, ZephyrAgent, ClaudeAgent, LlamaAgent, FalconAgent, GPT3BaseAgent, ConversationalGPTBaseAgent\n",
    "from transformers import set_seed\n",
    "\n",
    "# if __package__ is None:\n",
    "#     import sys\n",
    "#     from os import path\n",
    "#     p = path.dirname(path.dirname(path.dirname(path.abspath(__file__))))\n",
    "#     sys.path.insert(0,p)\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from personality.consts import p2_descriptions as their_p2_descriptions, naive_prompt\n",
    "from personality.descriptions import p2_descriptions as our_p2_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior = load_dataset('allenai/SimpleToM', 'behavior-qa')['test'].to_pandas()\n",
    "judgement = load_dataset('allenai/SimpleToM', 'judgment-qa')['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../data/simpletom/simpletom.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior['type'] = 'behavior'\n",
    "judgement['type'] = 'judgement'\n",
    "df = pd.concat([behavior, judgement]).reset_index(drop=True)\n",
    "df.to_csv('../../data/simpletom/simpletom.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EXAMPLES_TO_PARSE = None # set as None to use all, set >0 for prototyping\n",
    "PROJECT_HOME = \"\" #Path(__file__).parent.resolve()\n",
    "DATA_DIR = 'data'\n",
    "DATA_DIR_PATH = os.path.join(PROJECT_HOME, DATA_DIR)\n",
    "RANDOM_SEED = 99\n",
    "random.seed(RANDOM_SEED)\n",
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, texts, args):\n",
    "        self.texts = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgent():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.eval_dir_path = os.path.join(PROJECT_HOME, 'outs', 'simpletom')\n",
    "        if args.p2_source is not None:\n",
    "            self.eval_dir_path = os.path.join(self.eval_dir_path, args.p2_source)\n",
    "            self.p2_descriptions = self.get_descriptions()\n",
    "        if MAX_EXAMPLES_TO_PARSE is not None:\n",
    "            self.eval_dir_path = os.path.join(self.eval_dir_path, 'sample')\n",
    "        self.output_filename_suffix = '_{}_p-{}.json'.format(self.args.model, str(self.args.personality))\n",
    "        self.load_df()\n",
    "        self.setup_df()\n",
    "        self.model = self.load_model()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def get_descriptions(self):\n",
    "        if self.args.p2_source=='p2_ours':\n",
    "            return our_p2_descriptions\n",
    "        elif self.args.p2_source=='p2_theirs':\n",
    "            return their_p2_descriptions\n",
    "        elif self.args.p2_source=='naive':\n",
    "            return naive_prompt\n",
    "        else:\n",
    "            raise NotImplementedError(f'{self.args.p2_source} source is not defined.')\n",
    "    \n",
    "    def load_df(self):\n",
    "        self.df = pd.read_csv(os.path.join(PROJECT_HOME, 'data', 'simpeltom','simpletom.csv'))\n",
    "\n",
    "    def respond(self, prompt):\n",
    "        response = self.model.interact(prompt)\n",
    "        return response\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.args.model=='gpt-3.5-turbo-instruct':\n",
    "            model = GPT3BaseAgent({'engine': self.args.model, 'temperature': 0, 'top_p': 0.95, 'frequency_penalty': 0.0, 'presence_penalty': 0.0})\n",
    "        elif self.args.model=='gpt-3.5-turbo-1106':\n",
    "            model = ConversationalGPTBaseAgent({'model': self.args.model, 'temperature': 0, 'top_p': 0.95, 'frequency_penalty': 0.0, 'presence_penalty': 0.0})\n",
    "        elif self.args.model.startswith('flan-t5'):\n",
    "            model = FlanT5Agent(self.args)\n",
    "        elif self.args.model.startswith('flan-ul2'):\n",
    "            model = FlanUL2Agent(self.args)\n",
    "        elif self.args.model.startswith(\"anthropic\"):\n",
    "            model = ClaudeAgent({'model_name': self.args.model, 'temperature': 0, 'top_p': 0.95, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'max_tokens_to_sample': 700})\n",
    "        # elif self.args.model.endswith('-tg'):\n",
    "        #     model = TogetherAIAgent(self.args.__dict__)\n",
    "        elif self.args.model.startswith('mistral'):\n",
    "            model = MistralAIAgent(self.args)\n",
    "        elif self.args.model.startswith('zephyr'):\n",
    "            model = ZephyrAgent(self.args)\n",
    "        elif self.args.model.lower().startswith('llama'):\n",
    "            model = LlamaAgent(self.args)\n",
    "        elif self.args.model.startswith('falcon'):\n",
    "            model = FalconAgent(self.args)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return model\n",
    "\n",
    "    def evaluate_response(self, responses):\n",
    "        print(\"Running evaluation...\")\n",
    "\n",
    "        count = {'A' : 0, 'B' : 0, 'UNK' : 0}\n",
    "\n",
    "        assert len(self.df) == len(responses), \"Number of questions and model predictions should be the same.\"\n",
    "\n",
    "        for loc, res in enumerate(responses):\n",
    "            res = res.split('Answer:')[-1].strip().split('\\n')[0]\n",
    "            # choice = re.search(r'\\([abcdeABCDE]\\)', res, flags = 0).group()[1].upper()\n",
    "            choice = re.search(r'[abcdeABCDE][^a-zA-Z]{0,}', res, flags = 0)\n",
    "            if choice is None:\n",
    "                count[\"UNK\"] += 1\n",
    "            else:\n",
    "                choice = choice.group()[0].upper()\n",
    "                count[choice] += 1\n",
    "\n",
    "                row = self.dt_df.loc[loc]\n",
    "                label = row['label_dt']\n",
    "                #label_raw = row['label_raw']\n",
    "                # key = row['key']\n",
    "                # score = SCORES[choice]\n",
    "\n",
    "                # if key == 1:\n",
    "                #     traits[label].append(score)\n",
    "                # else:\n",
    "                #     traits[label].append(6 - score)\n",
    "\n",
    "        return (traits, count)\n",
    "\n",
    "    def run_reports(self, evaluated_response):\n",
    "        \"\"\"\n",
    "        Create report after scoring and analyzing the results        \n",
    "        \"\"\"\n",
    "        traits, count = evaluated_response\n",
    "\n",
    "        mean  = {}\n",
    "        std  = {}\n",
    "        for key, item in traits.items():\n",
    "            mean[key] = np.mean(np.array(item))\n",
    "            std[key] = np.std(np.array(item))\n",
    "\n",
    "        return f'''{count}\\n\\nmean:\\n {sorted(mean.items(), key=lambda item:item[0])}\\n std:\\n {sorted(std.items(), key=lambda item:item[0])}'''\n",
    "\n",
    "    def dump_report_outputs(self, reports):\n",
    "        \"\"\"\n",
    "        Dump the reports and the evaluation outputs\n",
    "        \"\"\"\n",
    "        report_filename = \"REPORT\" + self.output_filename_suffix\n",
    "        with open(os.path.join(self.eval_dir_path, report_filename), 'w') as f:\n",
    "            json.dump(reports, f, indent=4)\n",
    "        print(\">>>>> REPORT filename: {}\".format(report_filename))\n",
    "\n",
    "    def setup_dt(self):\n",
    "        template = \"\"\"You will be given a two sentence story and then asked a question. There will be two options A and B. You have to choose one of the options as the answer. Answer only with A or B and no additional text or explanation.\n",
    "        Question: {}\n",
    "        Options:\n",
    "        A - {}\n",
    "        B - {}\n",
    "        \n",
    "        Answer:\"\"\"\n",
    "        inputs = []\n",
    "        for idx, row in self.df.iterrows():\n",
    "            inputs.append(template.format(row['story'], row['question'], row['choices']['text'][0], row['choices']['text'][1]))\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def parse_response(self, response):\n",
    "        if \"Answer:\" in response:\n",
    "            response = response.split(\"Answer:\")[-1].strip()\n",
    "        elif \"<ANSWER>\" in response:\n",
    "            match = re.search(r'<ANSWER>(.+?)</ANSWER>', response, re.DOTALL)\n",
    "            if match:\n",
    "                response = match.group(1)\n",
    "            else:\n",
    "                response = re.sub('<ANSWER>','',response)\n",
    "        elif \"Choose an answer from above:\" in response:\n",
    "            response = response.split(\"Choose an answer from above:\")[-1].strip()\n",
    "\n",
    "        return response\n",
    "\n",
    "    def get_last_savepoint(self):\n",
    "        responses_filename = \"model_responses\" + self.output_filename_suffix + \"l\" # jsonl\n",
    "        model_responses_filename_path = os.path.join(self.eval_dir_path, responses_filename)\n",
    "\n",
    "        # check if model outputs file exists\n",
    "        if os.path.exists(model_responses_filename_path):\n",
    "            print(\"File {} exists. Reading responses from file...\".format(model_responses_filename_path))\n",
    "            df = pd.read_json(model_responses_filename_path, lines=True)\n",
    "            if len(df) > 0:\n",
    "                last_idx = df.iloc[-1]['index']\n",
    "                model_responses = df['response'].tolist()\n",
    "            else:\n",
    "                last_idx = -1\n",
    "                model_responses = []\n",
    "        else:\n",
    "            last_idx = -1\n",
    "            model_responses = []\n",
    "        \n",
    "        return last_idx, model_responses, model_responses_filename_path\n",
    "\n",
    "    def run_batch_inference(self):\n",
    "        mpi_dataset = SimpleDataset(self.inputs, self.args)\n",
    "        loader = DataLoader(mpi_dataset, batch_size=self.args.batch_size)\n",
    "\n",
    "        model_responses = []\n",
    "        print(\"Generating responses...\")\n",
    "        last_idx, model_responses, response_filename_path = self.get_last_savepoint()\n",
    "        if last_idx > 0:\n",
    "            last_idx = last_idx // self.args.batch_size\n",
    "        for batch_idx, batch in enumerate(tqdm(loader)):\n",
    "            if batch_idx <= last_idx:\n",
    "                continue\n",
    "\n",
    "            if self.args.personality is not None:\n",
    "                personality_prefix = \"Imagine you are someone that fits this description: \" + self.p2_descriptions[self.args.personality.title()] + \"\\n\\n\"\n",
    "                batch = [personality_prefix + b for b in batch]\n",
    "\n",
    "            responses = self.model.batch_interact(batch)\n",
    "\n",
    "            for idx, response in enumerate(responses):\n",
    "                response = self.parse_response(response)\n",
    "                model_responses.append(response)\n",
    "\n",
    "                # save the model responses in a file on the fly\n",
    "                with open(response_filename_path, 'a') as f:\n",
    "                    instance_for_dump = {'index': batch_idx * self.args.batch_size + idx, 'response': response, 'input_prompt': batch[idx]}\n",
    "                    json.dump(instance_for_dump, f)\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "        return model_responses\n",
    "\n",
    "    def run_inference(self):\n",
    "        target_data = self.inputs\n",
    "        model_responses = []\n",
    "\n",
    "        # check if the file exists\n",
    "        last_idx, model_responses, response_filename_path = self.get_last_savepoint()\n",
    "\n",
    "        print(\"Generating responses...\")\n",
    "        for idx, input_prompt in enumerate(tqdm(target_data)):\n",
    "            if idx <= last_idx:\n",
    "                continue\n",
    "\n",
    "            if self.args.personality is not None:\n",
    "                personality_prefix = \"Imagine you are someone that fits this description: \" + self.p2_descriptions[self.args.personality.title()] + \"\\n\\n\"\n",
    "                input_prompt = personality_prefix + input_prompt\n",
    "\n",
    "            response = self.model.interact(input_prompt)\n",
    "            response = self.parse_response(response)\n",
    "            model_responses.append(response)\n",
    "\n",
    "            # save the model responses in a file on the fly\n",
    "            with open(response_filename_path, 'a') as f:\n",
    "                json.dump({'index': idx, 'input_prompt': input_prompt, 'response': response}, f)\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "        return model_responses\n",
    "\n",
    "    def run(self):\n",
    "        os.makedirs(self.eval_dir_path, exist_ok=True)\n",
    "        if args.existing_response_file_name is None:\n",
    "            if self.args.model.startswith(\"gpt-\") or self.args.model.startswith(\"anthropic\") or self.args.model.startswith(\"text-\") or self.args.model.endswith(\"-tg\"):\n",
    "                model_responses = self.run_inference()\n",
    "            else:\n",
    "                model_responses = self.run_batch_inference()\n",
    "        else:\n",
    "            print(\">>> Reading responses from file...\")\n",
    "            model_responses = self.get_responses_from_file(self.args.existing_response_file_name)\n",
    "\n",
    "        evaluated_outputs = self.evaluate_response(model_responses)\n",
    "        reports = self.run_reports(evaluated_outputs)\n",
    "        self.dump_report_outputs(reports)\n",
    "\n",
    "    def get_responses_from_file(self, response_filename):\n",
    "        setup = response_filename.removeprefix(\"model_responses\").removesuffix(\".jsonl\")\n",
    "        assert setup == self.output_filename_suffix.removesuffix(\".json\"), \"The response file name does not match the output file name\"\n",
    "\n",
    "        response_file = os.path.join(self.eval_dir_path, response_filename)\n",
    "        df = pd.read_json(response_file, lines=True)\n",
    "        model_responses = df['response'].to_list()\n",
    "        return model_responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "                             [--model {flan-ul2,flan-t5-xxl,flan-t5-xl,Llama-2-7b-hf,Llama-2-7b-chat-hf,Llama-2-13b-hf,Llama-2-13b-chat-hf,zephyr-7b-alpha,zephyr-7b-beta,mistral,mistral-instruct,mpt-30b-instruct-tg,guanaco-33b-tg,anthropic.claude-v2:1,falcon-7b-instruct,gpt-3.5-turbo-instruct,gpt-3.5-turbo-1106}]\n",
      "                             [--batch-size BATCH_SIZE]\n",
      "                             [--existing-response-file-name EXISTING_RESPONSE_FILE_NAME]\n",
      "                             [--personality PERSONALITY]\n",
      "                             [--p2_source {naive,p2_theirs,p2_ours}]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/home/kokil/.local/share/jupyter/runtime/kernel-v337999f404f070e28920abb0915ca0d4ebff76029.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main(args):\n",
    "    evaluator = SimpleAgent(args)\n",
    "    evaluator.run()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='arguments for generating dialogues')\n",
    "    parser.add_argument('--model',\n",
    "                        type=str,\n",
    "                        default='gpt-4-0314',\n",
    "                        choices=[\n",
    "                            'flan-ul2', 'flan-t5-xxl', 'flan-t5-xl', \n",
    "                            'Llama-2-7b-hf', 'Llama-2-7b-chat-hf', 'Llama-2-13b-hf', 'Llama-2-13b-chat-hf',\n",
    "                            'zephyr-7b-alpha', 'zephyr-7b-beta', \n",
    "                            'mistral', 'mistral-instruct', \n",
    "                            'mpt-30b-instruct-tg', 'guanaco-33b-tg', \n",
    "                            'anthropic.claude-v2:1','falcon-7b-instruct',\n",
    "                            'gpt-3.5-turbo-instruct', 'gpt-3.5-turbo-1106'\n",
    "                            ],\n",
    "                        help='name of the model to run evaluation',\n",
    "    )\n",
    "    parser.add_argument('--batch-size',\n",
    "                        type=int,\n",
    "                        default=1,\n",
    "                        help='batch size for evaluation',\n",
    "    )\n",
    "    parser.add_argument('--existing-response-file-name',\n",
    "                        type=str,\n",
    "                        help='name of the response file that you want to recompute the report for',\n",
    "    )\n",
    "    parser.add_argument('--personality',\n",
    "                        type=str,\n",
    "                        default=None,\n",
    "                        help='whether to use personality or None',\n",
    "    )\n",
    "    parser.add_argument('--p2_source',\n",
    "                        type=str,\n",
    "                        default=None,\n",
    "                        choices=['naive','p2_theirs', 'p2_ours'],\n",
    "                        help='which personality description to use',\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
